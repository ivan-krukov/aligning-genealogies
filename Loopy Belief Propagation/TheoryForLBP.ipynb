{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Graphical Models and BP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Graphical models\n",
    "\n",
    "A graphical model is a compact representation of a collection of probability distributions. It consists of a graph $G = (V, E)$, directed or undirected, where each vertex $v \\in V$ is accosiated with a random variable. The edges of the graph represent statistical relationships between variables. There are several main types of graphical models:\n",
    "\n",
    "* Bayesian networks\n",
    "* Markov random fields\n",
    "* Factor graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayesian networks\n",
    "\n",
    "A Bayesian network is a directed graph. Each random variable $x_i$ in the network has an associated conditional probability distribution or local probabilistic model. A directed edge from $x_i$ to $x_j$ represents a conditional probability of the random variable, given its parents in the graph, namely $P(x_i\\ |\\ x_j)$. Bayesian network defines a joint probability distribution:\n",
    "\n",
    "$$\n",
    "P(x_1, \\dots, x_n) = \\prod_{i=1}^{n}P(x_i\\ |\\ \\mathbf{Par}(x_i))\n",
    "\\tag{1.1}\n",
    "$$\n",
    "\n",
    "**Example 1 (Bayesian network).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/1/1.1.png)\n",
    "<center>Fig 1.1: A simple Bayesian network. Figure is from [1]</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear to see, that probability distribution is as follows:\n",
    "\n",
    "$$\n",
    "P(P, T, I, X, S) = P(P)P(T)P(I|P,T)P(X|I)P(S|T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Markov random fields\n",
    "\n",
    "Markov Random Fields are based on undirected graphical models. As in a Bayesian network, the nodes in the graph represent the variables. An associated probability distribution factorizes into functions, each function associated with a clique of the graph:\n",
    "\n",
    "$$\n",
    "P(x_1, ..., x_n) = Z^{-1} \\prod_{C \\in \\mathcal{C}} \\psi_C(x_C)\n",
    "\\tag{1.2}\n",
    "$$\n",
    "\n",
    "where Z is a constant chosen to ensure that the distribution is normalized. The set $\\mathcal{C}$ is often taken to be the set of all maximal cliques of the graph. For a general undirected graph the compatibility functions $\\psi_C$ need not have any obvious or direct relation to probabilities or conditional distributions defined over the graph cliques. \n",
    "\n",
    "A special case of Markov Random Fields is the pair wise Markov Random Field where the probability distribution factors into functions of two variables and the edges of the graph correspond to constraints between pairs of variables.\n",
    "\n",
    "**Example 2 (pairwise MRF).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1/1.2.png)\n",
    "<center>Fig 1.2: A simple  pairwise Markov Network. Figure is from [1]</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear to see, that probability distribution is as follows:\n",
    "\n",
    "$$\n",
    "P(P_1, P_2, P_3, P_4) = \\prod_{(ij)} \\psi_{(ij)}(P_i, P_j) \\prod_{i=1}^{4} \\psi_i(P_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Factor graphs\n",
    "\n",
    "The most general parameterization is a factor graph. A factor graph simplify displaying factorization of the probability distribution. Factor graphs explicitly draw out the factors in a factorization of the joint probability. It is possible to convert arbitrary pairwise MRF or Bayesian network into equivalent factor graph.\n",
    "\n",
    "**Defenition 1.** *Factor graph* is a pair $\\mathcal{F} = (G, \\{f_1, \\dots, f_n\\})$, where\n",
    "\n",
    "* $G = (V, E)$ is an undirected bipartite graph such that $V = X \\cup F$, where $X \\cup F = \\emptyset$. The nodes $X$ are variable nodes, while the nodes F are factor nodes.\n",
    "\n",
    "* Further, $f_1, \\dots ,f_n$ are positive functions and the number of functions equals the number of nodes in $F = \\{F_1, \\dots , F_n\\}$. Each node $F_i$ is associated with the corresponding function $f_i$ and each $f_i$ is a function of the neighboring variables of $F_i$, i.e. $f_i = f_i(\\mathbf{Nb}(F_i))$.\n",
    "\n",
    "The joint probability distribution of a factor graph of N variables with M functions can be found as follows:\n",
    "\n",
    "$$\n",
    "P(x_1, \\dots, x_n) = Z^{-1} \\prod_{a=1}^M \\psi(\\{x\\}_a)\n",
    "\\tag{1.3}\n",
    "$$\n",
    "\n",
    "The constant Z is a normalization constant. \n",
    "\n",
    "**Example 3 (Converting a pairwise MRF into a factor graph).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1/1.3.png)\n",
    "<center>Fig 1.3: Converting a pairwise MRF into a factor graph. First figure is from [1]</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square nodes are function nodes; they connect to variables that are the arguments of those functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Inference in graphical models\n",
    "\n",
    "Both directed and undirected graphical models represent a full joint probability distribution. It is important to solve the following computational inference problems:\n",
    "\n",
    "* computing the marginal distribution $P(\\{x\\}_A)$ over a particular subset $A \\in V$ of nodes. Mathematically, marginal probabilities are defined in terms of sums over all the possible states of all the other nodes in the system. For example, marginal probability of the last node can be found as follows:\n",
    "\n",
    "$$\n",
    "P(x_n) = \\sum_{x_1} \\dots \\sum_{x_{n-1}} P(x_1, \\dots, x_n)\n",
    "\\tag{2.1}\n",
    "$$\n",
    "\n",
    "* computing the conditional distribution $P(\\{x\\}_A\\ |\\ \\{x\\}_B )$, where $A \\cap B = \\emptyset$ and $A, B \\in V$\n",
    "* computing the maximum a posteriori (MAP). The task is to find the most likely assignment to the variables in $A \\in V$ given the evidence $B \\in V$: $\\text{argmax}_{\\{x\\}_A} P(\\{x\\}_A\\ |\\ \\{x\\}_B)$\n",
    "* etc\n",
    "\n",
    "Marginal probabilities that are computed approximately are called beliefs. The belief at node $i$ is denoted by $b(x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Belief propagation\n",
    "\n",
    "Let's consider the following simple pairwise Markov Random Field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/1/3.1.png)\n",
    "<center>Fig 3.1: Pairwise MRF</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All inference tasks can be solved by directly evaluating the corresponding sums. Let’s marginalize the joint probability to find the marginal probability at node 5, $P(x_5)$. By defenition\n",
    "\n",
    "$$\n",
    "P(x_4) = Z^{-1} \\sum_{x_1, x_2, x_3, x_5, x_6, x_7} P(x_1, x_2, x_3, x_4, x_5, x_6, x_7)\n",
    "\\tag{3.1}\n",
    "$$\n",
    "\n",
    "Let's exploit the underlying graph structure:\n",
    "\n",
    "$$\n",
    "P(x_4) = Z^{-1} \\sum_{x_1, x_2, x_3, x_5, x_6, x_7} \\psi_1(x_1, x_4) \\psi_2(x_2, x_4) \\psi_3(x_3, x_4) \\psi_4(x_4, x_5) \\psi_5(x_5, x_6) \\psi_6(x_5, x_7)\n",
    "\\tag{3.2}\n",
    "$$\n",
    "\n",
    "This next step shows the main idea of belief propagation. Because of the modular structure of the joint probability, we can pass the summations through variables it doesn’t sum over. This gives the same marginalization result, but computed much more efficiently:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x_4) & = Z^{-1} \\sum_{x_1, x_2, x_3, x_5, x_6} \\psi_1(x_1, x_4) \\psi_2(x_2, x_4) \\psi_3(x_3, x_4) \\psi_4(x_4, x_5) \\psi_5(x_5, x_6) \\left[\\sum_{x_7} \\psi_6(x_5, x_7)\\right]=\\\\\n",
    "&= Z^{-1} \\sum_{x_1, x_2, x_3, x_5, x_6} \\psi_1(x_1, x_4) \\psi_2(x_2, x_4) \\psi_3(x_3, x_4) \\psi_4(x_4, x_5) \\psi_5(x_5, x_6) \\mu_{7 \\rightarrow 5} (x_5)=\\\\\n",
    "&= Z^{-1} \\sum_{x_1, x_2, x_3, x_5} \\psi_1(x_1, x_4) \\psi_2(x_2, x_4) \\psi_3(x_3, x_4) \\psi_4(x_4, x_5) \\left[\\sum_{x_6} \\psi_5(x_5, x_6)\\right] \\mu_{7 \\rightarrow 5} (x_5)=\\\\\n",
    "&= Z^{-1} \\sum_{x_1, x_2, x_3, x_5} \\psi_1(x_1, x_4) \\psi_2(x_2, x_4) \\psi_3(x_3, x_4) \\psi_4(x_4, x_5) \\mu_{6 \\rightarrow 5} (x_5) \\mu_{7 \\rightarrow 5} (x_5)=\\\\\n",
    "&= Z^{-1} \\sum_{x_1, x_2, x_3} \\psi_1(x_1, x_4) \\psi_2(x_2, x_4) \\psi_3(x_3, x_4) \\left[ \\sum_{x_5} \\psi_4(x_4, x_5) \\mu_{6 \\rightarrow 5} (x_5) \\mu_{7 \\rightarrow 5} (x_5) \\right] =\\\\\n",
    "&= Z^{-1} \\sum_{x_1, x_2, x_3} \\psi_1(x_1, x_4) \\psi_2(x_2, x_4) \\psi_3(x_3, x_4) \\mu_{5 \\rightarrow 4} (x_4)=\\\\\n",
    "&= Z^{-1} \\left[ \\sum_{x_1} \\psi_1(x_1, x_4) \\right] \\left[ \\sum_{x_2} \\psi_2(x_2, x_4) \\right] \\left[ \\sum_{x_3}\\psi_3(x_3, x_4) \\right] \\mu_{5 \\rightarrow 4} (x_4)=\\\\\n",
    "&= Z^{-1} \\mu_{1 \\rightarrow 4} (x_4) \\mu_{2 \\rightarrow 4} (x_4) \\mu_{3 \\rightarrow 4} (x_4) \\mu_{5 \\rightarrow 4} (x_4)\n",
    "\\end{aligned}\n",
    "\\tag{3.3}\n",
    "$$\n",
    "\n",
    "The normalization constant $Z$ can be determined as follows:\n",
    "\n",
    "$$\n",
    "Z = \\sum_{x_4} \\mu_{1 \\rightarrow 4} (x_4) \\mu_{2 \\rightarrow 4} (x_4) \\mu_{3 \\rightarrow 4} (x_4) \\mu_{5 \\rightarrow 4} (x_4)\n",
    "\\tag{3.4}\n",
    "$$\n",
    "\n",
    "**Defenition 3.1.** $\\mu_{i\\rightarrow j}$ or so-called message is a reusable partial sum for for finding the marginal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/1/3.2.png)\n",
    "<center>Fig 3.2: Summary of the messages</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sum-product for MRF\n",
    "\n",
    "The generalization of the procedure above is exactly belief propagation or sum-product algorithm.\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\mu_{i\\rightarrow j} (x_j) & = \\sum_{x_i} \\psi_{(ij)} (x_i, x_j) \\prod_{k \\in \\mathbf{Nb}(x_i)\\setminus \\{x_j\\}} \\mu_{k\\rightarrow i}(x_i) \\\\\n",
    "P(x_i) & = Z^{-1} \\prod_{j\\in \\mathbf{Nb}(x_i)} \\mu_{j\\rightarrow i} (x_i)\n",
    "\\end{aligned}\n",
    "}\n",
    "\\tag{3.5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sum-product for Factor graphs\n",
    "\n",
    "In fact, all different BP algorithms are mathematically equivalnt. The BP for factor graphs is described using two kinds of messages: from factors to variables $(\\mu_{x_i\\rightarrow f_j})$ and from variables to factors $(\\mu_{f_i\\rightarrow x_j})$.\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\mu_{x_i\\rightarrow f_j} & = \\prod_{f_k \\in \\mathbf{Nb}(x_i)\\setminus\\{f_j\\}} \\mu_{f_k \\rightarrow x_i} \\\\\n",
    "\\mu_{f_i\\rightarrow x_j} & = \\sum_{\\mathbf{NB}(f_i)\\setminus \\{x_j\\}} \\left( f_i \\prod_{x_k \\in \\mathbf{Nb}(f_i)\\setminus \\{x_j\\}} \\mu_{x_k \\rightarrow f_i}(x_k) \\right) \\\\\n",
    "P(x_i) & = Z^{-1} \\prod_{f_k \\in \\mathbf{Nb}(x_i)} \\mu_{f_k \\rightarrow x_i} (x_i)\n",
    "\\end{aligned}\n",
    "}\n",
    "\\tag{3.6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Max-product algorithm\n",
    "\n",
    "**Defenition 2.** The *max-marginal* at each node is defined as follows:\n",
    "\n",
    "$$\n",
    "\\bar{p}_{x_i}(x_i) = \\max_{x_j: j \\neq i} p_{\\{x\\}} (\\{x\\})\n",
    "$$\n",
    "\n",
    "If there are no ties at each node, then $x_i^* \\in \\text{argmax}_{x_i} \\bar{p}_{x_i}(x_i)$ is the unique global MAP assignment.\n",
    "\n",
    "The idea behind max-product algorithm is nearly the same as for sum-product. For computing the maximum a posteriori (MAP) instead of summing over the states of other nodes, we should find the ```max``` over those states. The ```max``` operator passes through variables just as the summation sign did, and we get the following set of equations.\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\mu_{i\\rightarrow j} (x_j) & = \\max_{x_i} \\psi_{(ij)} (x_i, x_j) \\prod_{k \\in \\mathbf{Nb}(x_i)\\setminus \\{x_j\\}} \\mu_{k\\rightarrow i}(x_i) \\\\\n",
    "\\bar{p}_{x_i}(x_i) & = \\prod_{j\\in \\mathbf{Nb}(x_i)} \\mu_{j\\rightarrow i} (x_i)\n",
    "\\end{aligned}\n",
    "}\n",
    "\\tag{3.7}\n",
    "$$\n",
    "\n",
    "To obtain variable assignments we should store ```argmax``` for each message:\n",
    "\n",
    "$$\n",
    "\\delta_{i\\rightarrow j} = \\text{argmax}_{x_i} \\psi_{(ij)} (x_i, x_j) \\prod_{k \\in \\mathbf{Nb}(x_i)\\setminus \\{x_j\\}} \\mu_{k\\rightarrow i}(x_i)\n",
    "\\tag{3.8}\n",
    "$$\n",
    "\n",
    "And then bactrack as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_i^* &\\in \\text{argmax}_{x_i} \\bar{p}_{x_i}(x_i) \\\\\n",
    "x_j^* &= \\delta_{j\\rightarrow i} (x_i^*),\\ \\forall j \\in V\\setminus\\{i\\}\n",
    "\\end{aligned}\n",
    "\\tag{3.9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Max-sum algorithm\n",
    "\n",
    "To prevent numerical underflow/overflow one might perform all computations in log sapce. Let's denote $\\tilde{m}_{i\\rightarrow j} (x_j) = \\ln m_{i\\rightarrow j} (x_j)$ and $\\tilde{\\psi}_{ij} (x_i, x_j) = \\ln \\psi_{ij} (x_i, x_j)$. \n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\tilde{m}_{i\\rightarrow j} (x_j) & = \\max_{x_i}\n",
    "\\left\\{\n",
    "\\tilde{\\psi}_{ij} (x_i, x_j) + \\sum_{k \\in \\mathbf{Nb}(x_i)\\setminus \\{x_j\\}} \\tilde{m}_{k\\rightarrow i}(x_i)\n",
    "\\right\\} \\\\\n",
    "\\bar{p}_{x_i}(x_i) & = \\exp\n",
    "\\left\\{\n",
    "\\sum_{j\\in \\mathbf{Nb}(x_i)} \\tilde{m}_{k\\rightarrow i} (x_i)\n",
    "\\right\\}\n",
    "\\end{aligned}\n",
    "}\n",
    "\\tag{3.10}\n",
    "$$\n",
    "\n",
    "All multiplications are now replaced by additions. Now equations take the so-called MS form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Complexity & correctness\n",
    "\n",
    "Let's return to the Eq. (3.1). Suppose, that each $x_i$ has $k$ number of statets. If we know nothing about the structure of the joint probability, evaluation of $P(x_4)$ requires $k^6$ computations. In the more general case, we need $\\mathcal{O}(k^{|V|})$ summations to compute the desired marginal probability. That's why this approach becomes intractable for PGMs with many variables.\n",
    "\n",
    "**Claim 4.1.** Every type of inference in graphical models is NP-hard. Even simplest problem of computing the distribution over a single binary variable is NP-hard.\n",
    "\n",
    "Fortunately, factorization of the sum as we did in Eq. (3.3) reduces the total complexity to $\\mathcal{O}(|V|\\ k^2)$, where $|V|$ - number of nodes in the graph. However, it may not converge in many cases. The following result can be established via induction.\n",
    "\n",
    "**Claim 4.2.** The BP algorithm is exact if the topology of the PGM is that of a tree or a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Loopy belief propagation\n",
    "\n",
    "Loopy belief propagation may not converge on graphs with cycles, however in many cases it provides quite good approximation. There is still the question of what update rules we use to recompute the messages. For example, the following simple schedule is possible:\n",
    "\n",
    "1. initialize all messages $\\mu_{i\\rightarrow j}^{(0)} = 1\\ \\forall(i,j)\\in E$\n",
    "2. for $t \\in \\{1, \\cdots, t_{max}\\}$\n",
    "\n",
    "$\\qquad$ for all $(i,j)\\in E$ compute\n",
    "\n",
    "$\\qquad\\qquad$ $\\mu_{i\\rightarrow j}^{(t+1)} = \\sum_{x_i} \\psi_{(ij)} (x_i, x_j) \\prod_{k \\in \\mathbf{Nb}(x_i)\\setminus \\{x_j\\}} \\mu_{k\\rightarrow i}^{(t)}(x_i)$\n",
    "\n",
    "3. compute beliefs $b(x_i) = Z^{-1} \\prod_{j\\in \\mathbf{Nb}(x_i)} \\mu_{j\\rightarrow i}^{(t_{max}+1)} (x_i)$\n",
    "4. normalize $b(x_i)$\n",
    "\n",
    "Other schedules for message-passing are possible [10,11]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
